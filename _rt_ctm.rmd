---
    

---


```{r setup, include = F}
library(dplyr)
library(readr)
library(tm)
library(stringr)
library(quanteda)
library(topicmodels)

twitter_stopwords <- c("amp", "rt", "co", "t", "cdcgov", "@CDCgov")

rt <- read_rds("data/reply_tweets.Rds")
```


```{r rt-sample}
rt_sample <- rt %>%
    head(20000)
```

```{r init-corpus}
rts_corpus <- corpus(
    rt_sample,
    text_field = "text",
    docid_field = "tweet_id"
)

rts_tokens <- tokens(
    x = rts_corpus,
    remove_numbers = T,
    remove_url = T,
    remove_symbols = T,
    remove_punct = T,
    remove_separators = T,
    split_hyphens = T
) %>%
    tokens_select(
        pattern = c(
            "(@[A-Za-z_]+)+"
        ),
        valuetype = "regex",
        selection = "remove"
    ) %>%
    tokens_select(
        pattern = c(
            stopwords::stopwords(language = "en"),
            stopwords::stopwords(source = "smart"),
            twitter_stopwords
        ),
        selection = "remove",
    ) %>%
    tokens_ngrams(
        n = c(1L, 2L)
    )
```

```{r rts-dfm}
rts_dfm <- quanteda::dfm(
    x = rts_tokens
)

rts_dfm <- dfm_subset(
    x = rts_dfm,
    subset = ntoken(rts_dfm) > 0
)
```

```{r rts-LDA, eval = F}
rts_lda <- LDA(
    x = rts_dfm,
    k = 12,
    method = "Gibbs",
    control = list(
        alpha = 0.1,
        iter = 1000,
        verbose = 10
    )
)
```

```{r rts-lda-terms, eval = F}
rts_lda_topics <- topicmodels::topics(rts_lda)
rts_lda_terms <- topicmodels::terms(rts_lda, 20)
```

```{r rts-ctm, eval = F}
rts_ctm <- CTM(
    x = rts_dfm,
    k = 5,
    control = list(
        var = list(tol = 10^-4),
        em = list(tol = 10^-3),
        cg = list(
            iter.max = 300
        )
    )
)
```