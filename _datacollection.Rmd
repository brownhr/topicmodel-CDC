# Data

The data for this study consists of a corpus of approximately 300,000 Tweets regarding the Centers for Disease Control and Prevention (CDC). As the CDC disseminates information regarding vaccinations, face coverings (masks), and general public health, tweets made in reply to the CDC (Reply Tweets) often follow the same topic of discussion. The data collection methods defined below explain the workflow for obtaining and cleaning the text corpus within R.

## Twitter API

Twitter data acquisition on a large scale is made possible through the Twitter API v2 Academic Research Access, a platform designed by Twitter for use in academic and scientific research [@noauthor_twitter_nodate]. Academic access differs from standard API access in that rather than limiting query results to only the last 7 days of public Tweets, the Academic Research Access allows for full-archive searching, as well as much a higher limit on the number of monthly Tweets able to be requested.

## academictwitteR

The `academictwitteR` package for R allows the user to access the Academic Research archive from the Twitter API v2 and was developed in @barrie_academictwitter_2021. CDC Tweets were gathered by querying "`from:CDCgov`" from Jan. 1, 2019, to Sep. 21, 2021; Reply Tweets were acquired from the same time frame with the query "`@CDCgov`", meaning the text of the Reply Tweet contained the phrase "\@CDCgov", the name of the CDC's Twitter handle.


## Conversation ID

Tweets obtained from the `academictwitteR` R package also contain a unique identifier, `conversation_id`, which is an attribute generated by Twitter to keep track of conversation threads, allowing for more effective modeling and network analysis on "nonlinear" conversations. [@barrie_academictwitter_2021; @noauthor_twitter_nodate].


## Data Preprocessing

As the corpus contains raw, unfiltered text, steps must be taken to clean and process the data into a format suitable for topic modeling; the methods for processing the text data are outlined below. These steps are performed to ensure accurate results from topic modeling; for example, as the CDC's Twitter handle *\@CDCgov" is, by definition, included in all Reply Tweets, it is removed from the analysis so that results are not skewed in a particular direction.


### Tokenization

*Tokenization* refers to the act of splitting a length of text (a `string`) into individual segments or words, which are most often split by punctuation and whitespace. These tokens consist primarily of unigrams (individual words), but can also be extended to bigrams (two words occurring in order together). Trigrams (three words in-a-row) were omitted as, while they offer significant contextual and semantic information, they occurred too infrequently to be particularly useful; this is a noted issue in *short-text* datasets such as Twitter [@bao_joint_2009; @debortoli_text_2016; @hong_empirical_2010; @ostrowski_using_2015].

### Stopword Removal

Stopwords are often defined as the most frequently used words which do not offer very much useful information, such as "the", "and", or "at", etc. Stopwords were removed using the `textmineR` and `stopwords` R packages; additionally, hyperlinks and Twitter handle mentions were considered "stopwords" in this analysis as they do not offer semantic value. Other Twitter-specific stopwords include tokens like *RT* ("Retweet"), *amp* (a mis-encoding of the $\&$ character), *http*, and *tco*.

Once stopwords, links, and Handle mentions were removed, many Tweets (i.e., spam Tweets) did not contain any text and were thereafter removed. Additionally, extraneous duplicate Tweets by the same author replying to the same CDC Tweet were removed.

### Lemmatization

Lemmatization is done to produce the base form of a word (e.g., *running*, *runs*, and *run* all converge to the base form *run*). This is done to preserve the semantic meaning of words while maintaining a consistent "dictionary" of tokens; for example, with lemmatization, *mask mandates* and *mask mandate* produce the same lemmatized bigram, `mask_mandat`, which allows for comparison of words of the same effective meaning which differ only in tense or grammatical number. Lemmatization was performed with use of the `SnowballC` R package, which provides several word-stemming algorithms.


